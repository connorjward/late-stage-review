\section{Background}
\label{sec:background}

\subsection{Meshes}
\label{sec:background_mesh}

When solving \glspl{pde} numerically, the domain of interest is discretised into a \textit{mesh} composed of many polytopal cells (e.g. simplices, quadrilaterals or hexahedra).
These cells, along with the other topological entities with lower dimension (i.e. facets, edges and vertices), can have \glspl{dof} stored on them, representing some physically quantity distributed over the domain.
For higher order methods, multiple \glspl{dof} may be stored on each entity at \textit{nodes} (shown in Figure~\ref{fig:orient}).
Similarly, for vector-valued quantities, multiple \glspl{dof} may be stored at the same node.

Meshes can be classified as either \textit{structured}, \textit{unstructured} or \textit{partially-structured} depending on the regularity of the mesh connectivity.
Structured and partially-structured meshes tend to have a small performance advantage over unstructured meshes (see Section~\ref{sec:background_structure}).

In software, meshes are usually represented as sets of topological entities, and the library provides a query language to identify subsets, determine connectivity between sets and access data stored on each entity.
Library users, however, can have disparate needs for the library (e.g. mesh hierarchies, adaptivity, interoperability with mesh generators) and so designing a suitable abstraction with enough features while still remaining performant is a challenge.

One approach for storing a mesh is to treat the topological entities as sets and queries as a relational database~\cite{tautgesMOABMeshOrientedDatabase2004}.
Whilst this lowering of the abstraction lets the software utilise existing database optimisations, it can also obscure some geometric information and make dimension-independent programming more challenging~\cite{knepleyMeshAlgorithmsPDE2009}.

\subsubsection{DMPlex}
\label{sec:background_mesh_dmplex}

\begin{figure}
  \centering
  \begin{subfigure}{0.45\textwidth}
    \begin{tikzpicture}
      \tkzDefPoint(0,2){v0}
      \tkzDefPoint(2,4){v1}
      \tkzDefPoint(2,0){v2}
      \tkzDefPoint(4,2){v3}

      \tkzDrawSegments(v0,v1 v0,v2 v1,v2 v1,v3 v2,v3)
      \tkzDrawPoints(v0,v1,v2,v3)

      \tkzDefBarycentricPoint(v0=1.2,v1=1,v2=1) \tkzGetPoint{c0}
      \tkzLabelPoint[centered](c0){1}
      \tkzDefBarycentricPoint(v1=1,v2=1,v3=1.2) \tkzGetPoint{c1}
      \tkzLabelPoint[centered](c1){2}

      \tkzLabelPoint[left](v0){3}
      \tkzLabelPoint[above](v1){4}
      \tkzLabelPoint[below](v2){5}
      \tkzLabelPoint[right](v3){6}

      \tkzLabelSegment[below left](v0,v2){7}
      \tkzLabelSegment[above left](v0,v1){8}
      \tkzLabelSegment[left](v1,v2){9}
      \tkzLabelSegment[above right](v1,v3){10}
      \tkzLabelSegment[below right](v2,v3){11}
    \end{tikzpicture}
  \end{subfigure}
  %
  \begin{subfigure}{0.45\textwidth}
    \centering
    \begin{tikzpicture}
      \basichasse
    \end{tikzpicture}
  \end{subfigure}
  \caption{
    An example mesh and its Hasse diagram representation.
    Note that the topological entities are numbered according to the DMPlex convention of first cells, then vertices, then faces.
  }
  \label{fig:hasse_diagram}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}{0.45\textwidth}
    \centering
    \begin{tikzpicture}
      \basichasse
      \drawpolygon 7,9;
      \draw [dashed] (1) circle [radius=15pt];
    \end{tikzpicture}
    \caption{$\cone(1) = \{7,8,9\}$}
  \end{subfigure}
  %
  \begin{subfigure}{0.45\textwidth}
    \centering
    \begin{tikzpicture}
      \basichasse
      \drawpolygon 1,2;
      \draw [dashed] (9) circle [radius=15pt];
    \end{tikzpicture}
    \caption{$\support(9) = \{1,2\}$}
  \end{subfigure}
  %
  \begin{subfigure}{0.45\textwidth}
    \centering
    \begin{tikzpicture}
      \basichasse
      \drawpolygon 1,7,3,5,9;
    \end{tikzpicture}
    \caption{$\closure(1) = \{1,3,4,5,7,8,9\}$}
  \end{subfigure}
  %
  \begin{subfigure}{0.45\textwidth}
    \centering
    \begin{tikzpicture}
      \basichasse
      \drawpolygon 4,10,2,1,8;
    \end{tikzpicture}
    \caption{$\plexstar(4) = \{4,8,9,10,1,2\}$}
  \end{subfigure}

  \caption{The possible DMPlex covering queries (applied to the Hasse diagram from Figure~\ref{fig:hasse_diagram}).}
  \label{fig:plex_restrictions}
\end{figure}

In this work we will focus on DMPlex~\cite{knepleyMeshAlgorithmsPDE2009,langeEfficientMeshManagement2016,knepleyUnstructuredOverlappingMesh2015}, the unstructured mesh abstraction used in the linear algebra library PETSc~\cite{petsc-web-page,petsc-user-ref,petsc-efficient}.
Rather than using a database, DMPlex represents meshes as a Hasse diagram (a.k.a. a \gls{poset}).
Every topological entity, termed a \textit{point}, is given a unique number and related via uni-directional \textit{arrows} describing the connectivity between points.
An example mesh and its associated Hasse diagram are shown in Figure~\ref{fig:hasse_diagram}.

To express topological queries in DMPlex, a number of functions, termed \textit{covering relations} are provided (Figure~\ref{fig:plex_restrictions}).
More complex queries may also be created by composing these covering relations.
For example, the query ``return all of the points in the closure of the cells touching facet $f$" would be expressed as $\closure(\support(f))$.
In Figure~\ref{fig:hasse_diagram}, $\closure(\support(9))$ would correspond to all of the points in the mesh.
Composed queries such as this that return a sizeable number of mesh points are frequently referred to as \textit{patches}.

Since topological entities are universally referred to as points, most of DMPlex is written in an entirely dimension-independent way.
It also has support for a wide range of operations useful when solving \glspl{pde} including: interoperability with mesh generation libraries, adaptivity, parallel mesh distribution, orientations (Section~\ref{sec:background_orientation}) and mesh hierarchies.

When writing a PETSc application, users do not store data directly on the mesh, but instead instantiate parallel vectors (\clang{Vecs}) by composing the mesh with a \clang{PetscSection}.
In PETSc, a \clang{PetscSection} is used to describe data layouts by associating each mesh point with a given number of \glspl{dof} (\clang{PetscSectionSetDof(point,ndofs)}), allowing offsets in the \clang{Vec} to be tabulated and accessed (\clang{PetscSectionGetOffset(point,&offset)}).

\subsection{Stencil computations}

Stencil computations are ubiquitous in the numerical solution of \glspl{pde}.
Given a (possibly unstructured) mesh along with data associated with certain topological entities of it, a stencil computation refers to the process of:
a) looping over some subset of topological entities,
b) reading data that is `close' to the current entity (a \textit{stencil}),
c) executing some computation (kernel) using the stencil as input, and
d) scattering the computation result back to some global data structure.
\todo{A picture would be nice to help illustrate this - like the PyOP2 one.}

An important example of a stencil computation is \textit{finite element assembly}, itself one of the stages of \gls{fem}.
\todo{Could easily write a lot more on FEM.}
In the assembly process, integrals are evaluated for each cell of the mesh and the results are incremented into a global matrix and vector.
For \gls{fem}, the stencil is defined to be all \glspl{dof} stored on the cell and its connected edges and vertices (a.k.a. the cell's \textit{closure} - see Section~\ref{sec:background_mesh_dmplex}).
For \gls{dg} methods the iteration set can sometimes be the facets of the mesh, and the stencils the closure `macro cell' formed of the two cells that touch the facet.

Although this work will largely focus specifically on \gls{fem}, the concept of a stencil computation applies equally to finite difference and finite volume methods.

\subsubsection{Existing software for stencil computations}
\label{sec:stencillang}

Given the abstraction's wide range of applications, a fair number of libraries exist for (at least partially) automating the work of developing a stencil application.
For the following we choose to classify these libraries according to whether they are `mesh-aware' or `mesh-oblivious'.
By mesh-aware we mean that the library is responsible for interacting with and querying the mesh whereas mesh-oblivious libraries are unaware of the mesh's specific implementation and responsibility for executing the right topological queries is passed to the library's user.
As we will discuss, there are advantages and drawbacks to both approaches.

Some notable examples of mesh-aware stencil libraries include the \glspl{dsl} Liszt~\cite{devitoLisztDomainSpecific2011}, Ebb~\cite{bernsteinEbbDSLPhysical2016} and Simit~\cite{kjolstadSimitLanguagePhysical2016}.
Liszt represents meshes as collections of \textit{sets}, with \textit{topological relations} between them.
It supports both distributed memory parallelism through MPI and shared memory parallelism with pthreads; GPUs are also supported.
Instead of explicitly being for unstructured meshes, Ebb takes a more abstract approach and opts for using database-style \textit{relations} to describe the connectivity of the mesh.
This permits Ebb to also handle structured meshes which are typically faster (see Section~\ref{sec:background_structure}).
Distributed memory parallelism is currently not supported.
Lastly, Simit represents unstructured meshes using \textit{hypergraphs}, which are in fact very similar to the \gls{poset} description used by DMPlex.
Like Ebb, distributed memory parallelism is not supported.

Having the library be mesh-aware, all three \glspl{dsl} are able to be highly expressive and flexible and entire applications can be written in fewer than 100 lines of code.
However, in order to accomplish this, each one has had to implement its own custom mesh implementation inevitably resulting in an increased maintenance burden for the developer and a reduced feature set compared to more widely used mesh frameworks such as DMPlex (Section~\ref{sec:background_mesh_dmplex}).

A good example of a mesh-oblivious stencil library is OP2~\cite{mudaligeOP2ActiveLibrary2012,mudaligeDesignInitialPerformance2013}.
Rather than being a \gls{dsl}, users write code including calls to the OP2 API which is then source-to-source translated into optimised code which can then be compiled.
Similar to Liszt, topological entities are represented as \textit{sets} and topological relations by \textit{maps} between sets.
Stencil computations are expressed as \textit{parallel loops} where the user declares the \textit{kernel} to be executed, the \textit{iteration set} and the \textit{arguments} to the kernel where each argument consists of a parallel vector (termed a \textit{dat}), an access descriptor (e.g. \clang{OP_READ} or \clang{OP_INC}) and an optional indirection map.

OP2 supports a wide range of backends and has support for distributed memory parallelism, in fact it has a clever solution for interleaving computation and communication to aid performance (Section~\ref{sec:impl_parallel_dats}).

Since OP2 is mesh-oblivious and therefore stores no information about the underlying mesh it is entirely agnostic to the specific mesh implementation, giving maximum flexibility to the user.
However, as a consequence the non-trivial burdens of determining the sets and tabulating the indirection maps are passed to the user resulting in more complicated and less readable code.

\subsubsection{\pyop2}

\pyop2 is a mesh-oblivious stencil library intended to be a Python implementation of OP2~\cite{rathgeberPyOP2HighLevelFramework2012}.
Since the fundamental OP2 concepts of sets, maps between sets and parallel loops are naturally the same, the main difference between the two libraries is that \pyop2 is a \textit{runtime implementation} whereas OP2 depends on static analysis and source-to-source code transformations.
Being a runtime implementation, \pyop2 has access to runtime-specific information that it can exploit to enable additional optimisations.

Beyond this core difference, \pyop2 also has a slightly different feature set to OP2:

\begin{itemize}
  \item
    Only distributed memory parallelism is supported.
  \item
    Parallel loops can assemble sparse matrices.
  \item
    Execution on GPUs is still a work-in-progress~\cite{fenics2021-kulkarni}.
  \item
    Iteration over extruded meshes is supported (Section~\ref{sec:background_structure}).
\end{itemize}

% briefly mention Firedrake TSFC UFL, loopy - context needed for later
\pyop2 is used extensively by Firedrake, a library for automating the solution of \glspl{pde} using \gls{fem}~\cite{rathgeberFiredrakeAutomatingFinite2016}.
\todo{Could add a load more detail if desired}
In Firedrake, users express their \gls{pde} of interest (in weak form) in near-mathematical notation using \gls{ufl}, a \gls{dsl} embedded in Python~\cite{alnaesUnifiedFormLanguage2014a}.
Firedrake can then compile this form using \gls{tsfc}~\cite{homolyaTSFCStructurePreservingForm2018} before marshalling both \pyop2 and PETSc to automate its solution.

One of the places where Firedrake uses \pyop2 is to assemble the global matrix and vector required by \gls{fem}.
To do this the compiled form is used as the \textit{kernel} for a \pyop2 parallel loop and the \textit{iteration set} is the cells of the mesh.

\subsection{Orienting degrees-of-freedom}
\label{sec:background_orientation}

\begin{figure}
  % macros
  \newcommand{\trianglestyles}{%
    \tikzstyle {segment} = [line width=1.2pt];
    \tikzstyle {dof} = [draw=black,line width=1.2pt];
    \tikzstyle {celldof} = [dof,fill=blue!50];
    \tikzstyle {edgedof} = [dof,fill=red!60];
    \tikzstyle {vertdof} = [dof,fill=yellow!80];
    \tikzstyle {doftext} = [font=\bf];
    \tikzstyle {vdof} = [-stealth,draw=red!60,line width=1.9];
  }

  \newcommand{\stylesegmentsone}{%
    \tkzSetUpStyle[
      postaction=decorate,
      decoration={
        markings,
        mark=at position .53 with {\arrow[very thick]{##1}},
      }
    ]{myarrow}
  }

  \newcommand{\stylesegmentstwo}{%
    \tkzSetUpStyle[
      postaction=decorate,
      decoration={
        markings,
        mark=at position .78 with {\arrow[very thick]{##1}},
        mark=at position .28 with {\arrow[very thick]{##1}},
      }
    ]{myarrow}
  }

  \newcommand{\deftriangle}{%
    \trianglestyles
    \tkzDefPoint(0,0){v0}
    \tkzDefShiftPoint[v0](0:3){v1}
    \tkzDefShiftPoint[v0](60:3){v2}
  }

  \newcommand{\drawtriangle}{%
    \deftriangle
    \tkzDrawSegment[myarrow=stealth,segment](v0,v1)
    \tkzDrawSegment[myarrow=stealth,segment](v1,v2)
    \tkzDrawSegment[myarrow=stealth,segment](v0,v2)
  }

  \newcommand{\drawflippedtriangle}{%
    \deftriangle
    \tkzDrawSegment[myarrow=stealth,segment](v0,v1)
    \tkzDrawSegment[myarrow=stealth,segment](v2,v1)
    \tkzDrawSegment[myarrow=stealth,segment](v0,v2)
  }

  % figure
  \centering
  \begin{subfigure}{0.45\textwidth}
    \centering
    \begin{tikzpicture}
      \stylesegmentsone
      \drawtriangle

      % vert dofs
      \filldraw [vertdof] (v0) node [doftext] {0} circle [radius=7pt];
      \filldraw [vertdof] (v1) node [doftext] {1} circle [radius=7pt];
      \filldraw [vertdof] (v2) node [doftext] {2} circle [radius=7pt];

      % edge dofs
      \tkzDefBarycentricPoint(v0=2.3,v1=1) \tkzGetPoint{e0d0}
      \filldraw [edgedof] (e0d0) node [doftext] {7} circle [radius=7pt];

      \tkzDefBarycentricPoint(v0=1,v1=2.3) \tkzGetPoint{e0d1}
      \filldraw [edgedof] (e0d1) node [doftext] {8} circle [radius=7pt];

      \tkzDefBarycentricPoint(v1=2.3,v2=1) \tkzGetPoint{e1d0}
      \filldraw [edgedof] (e1d0) node [doftext] {3} circle [radius=7pt];

      \tkzDefBarycentricPoint(v1=1,v2=2.3) \tkzGetPoint{e1d1}
      \filldraw [edgedof] (e1d1) node [doftext] {4} circle [radius=7pt];

      \tkzDefBarycentricPoint(v0=2.3,v2=1) \tkzGetPoint{e2d0}
      \filldraw [edgedof] (e2d0) node [doftext] {5} circle [radius=7pt];

      \tkzDefBarycentricPoint(v0=1,v2=2.3) \tkzGetPoint{e2d1}
      \filldraw [edgedof] (e2d1) node [doftext] {6} circle [radius=7pt];

      % cell dof
      \tkzDefBarycentricPoint(v0=1,v1=1,v2=1) \tkzGetPoint{c0d0}
      \filldraw [celldof] (c0d0) node [doftext] {9} circle [radius=7pt];
    \end{tikzpicture}
    \caption{Reference Lagrange finite element with polynomial degree 3.}
    \label{fig:orient_basic_ref}
  \end{subfigure}
  %
  \begin{subfigure}{0.45\textwidth}
    \centering
    \begin{tikzpicture}
      \stylesegmentsone
      \drawflippedtriangle

      % vert dofs
      \filldraw [vertdof] (v0) node [doftext] {0} circle [radius=7pt];
      \filldraw [vertdof] (v1) node [doftext] {1} circle [radius=7pt];
      \filldraw [vertdof] (v2) node [doftext] {2} circle [radius=7pt];

      % edge dofs
      \tkzDefBarycentricPoint(v0=2.3,v1=1) \tkzGetPoint{e0d0}
      \filldraw [edgedof] (e0d0) node [doftext] {7} circle [radius=7pt];

      \tkzDefBarycentricPoint(v0=1,v1=2.3) \tkzGetPoint{e0d1}
      \filldraw [edgedof] (e0d1) node [doftext] {8} circle [radius=7pt];

      \tkzDefBarycentricPoint(v1=2.3,v2=1) \tkzGetPoint{e1d0}
      \filldraw [edgedof] (e1d0) node [doftext] {4} circle [radius=7pt];

      \tkzDefBarycentricPoint(v1=1,v2=2.3) \tkzGetPoint{e1d1}
      \filldraw [edgedof] (e1d1) node [doftext] {3} circle [radius=7pt];

      \tkzDefBarycentricPoint(v0=2.3,v2=1) \tkzGetPoint{e2d0}
      \filldraw [edgedof] (e2d0) node [doftext] {5} circle [radius=7pt];

      \tkzDefBarycentricPoint(v0=1,v2=2.3) \tkzGetPoint{e2d1}
      \filldraw [edgedof] (e2d1) node [doftext] {6} circle [radius=7pt];

      % cell dof
      \tkzDefBarycentricPoint(v0=1,v1=1,v2=1) \tkzGetPoint{c0d0}
      \filldraw [celldof] (c0d0) node [doftext] {9} circle [radius=7pt];
    \end{tikzpicture}
    \caption{Degree 3 Lagrange finite element with an edge flipped.}
    \label{fig:orient_basic_flip}
  \end{subfigure}

  \vspace{1em}

  \begin{subfigure}{0.45\textwidth}
    \centering
    \begin{tikzpicture}
      \stylesegmentstwo
      \drawtriangle

      % edge dofs
      \tkzDefBarycentricPoint(v0=1,v1=1) \tkzGetPoint{e0d0}
      \tkzDefShiftPoint[e0d0](-90:.8){e0d0v}
      \draw [vdof] (e0d0) -- (e0d0v);
      \filldraw [edgedof] (e0d0) node [doftext] {2} circle [radius=7pt];

      \tkzDefBarycentricPoint(v0=1,v2=1) \tkzGetPoint{e1d0}
      \tkzDefShiftPoint[e1d0](150:.8){e1d0v}
      \draw [vdof] (e1d0) -- (e1d0v);
      \filldraw [edgedof] (e1d0) node [doftext] {1} circle [radius=7pt];

      \tkzDefBarycentricPoint(v1=1,v2=1) \tkzGetPoint{e2d0}
      \tkzDefShiftPoint[e2d0](210:.8){e2d0v}
      \draw [vdof] (e2d0) -- (e2d0v);
      \filldraw [edgedof] (e2d0) node [doftext] {0} circle [radius=7pt];
    \end{tikzpicture}
    \caption{Reference Raviart-Thomas finite element.}
    \label{fig:orient_vector_ref}
  \end{subfigure}
  %
  \begin{subfigure}{0.45\textwidth}
    \centering
    \begin{tikzpicture}
      \stylesegmentstwo
      \drawflippedtriangle

      % edge dofs
      \tkzDefBarycentricPoint(v0=1,v1=1) \tkzGetPoint{e0d0}
      \tkzDefShiftPoint[e0d0](-90:.8){e0d0v}
      \draw [vdof] (e0d0) -- (e0d0v);
      \filldraw [edgedof] (e0d0) node [doftext] {2} circle [radius=7pt];

      \tkzDefBarycentricPoint(v0=1,v2=1) \tkzGetPoint{e1d0}
      \tkzDefShiftPoint[e1d0](150:.8){e1d0v}
      \draw [vdof] (e1d0) -- (e1d0v);
      \filldraw [edgedof] (e1d0) node [doftext] {1} circle [radius=7pt];

      \tkzDefBarycentricPoint(v1=1,v2=1) \tkzGetPoint{e2d0}
      \tkzDefShiftPoint[e2d0](30:.8){e2d0v}
      \draw [vdof] (e2d0) -- (e2d0v);
      \filldraw [edgedof] (e2d0) node [doftext] {0} circle [radius=7pt];
    \end{tikzpicture}
    \caption{Raviart-Thomas element with an edge flipped.}
    \label{fig:orient_vector_flip}
  \end{subfigure}

  \caption{
    The effect of edge flips on both scalar- and vector-valued finite elements on triangles.
    Cell, edge and vertex \glspl{dof} are shown in blue, red and yellow respectively.
  }
  \label{fig:orient}
\end{figure}

When writing the local kernel for a stencil computation, the process is considerably simplified by assuming that the data being passed in from the iteration engine (here \pyop3) is in some canonical ordering.
Unfortunately, guaranteeing such an ordering a priori is often difficult as entities in the stencil may have different relative orientations.
To give an example, with an arbitrary global numbering of vertices, it is possible for a mesh to contain cells with `flipped' edges relative to their canonical orientation.
If multiple \glspl{dof} are stored along the edge then this will result in reading them in the wrong order, breaking the code.
An example of this for triangles is shown in Figure~\ref{fig:orient}.
Figure~\ref{fig:orient_basic_ref} shows the canonical ordering for a degree 3 Lagrange finite element.
Note how flipping an edge (Figure~\ref{fig:orient_basic_flip}) results in a permutation of the stencil \glspl{dof}.

The situation is further complicated when the \glspl{dof} stored at each node are vector-valued, for example with \hdiv and \hcurl conforming elements in \gls{fem}.
With vector \glspl{dof} it can happen that, as well as possibly being out of order, the loaded vectors can `point' in the wrong direction, requiring the application of some transformation to achieve a canonical representation.
For example, in Figures~\ref{fig:orient_vector_ref} and~\ref{fig:orient_vector_flip} one can see that the effect of flipping an edge inverts the direction of the vector \glspl{dof}, so they must be multiplied by -1 to return to canonical.
Things get even more difficult in 3D because vector-valued \glspl{dof} on facets can be defined relative to the two tangent vectors of the face.
The transformation back to canonical therefore can require rotations (via $2\times2$ matrices) as well as flips.
\todo{Serendipity elements also require complicated transformations using a full, dense matrix.}

\subsubsection{A partial solution: Mesh renumbering}

The approach used to `fix' the orientation issue used in most finite element libraries is to exploit the fact that, for a variety of cell types, it is possible to determine a global numbering of the vertices of the mesh such that all cells have the same orientation.
In particular this has been shown to work for simplices~\cite{rognesEfficientAssemblyDiv2010} and quadrilaterals~\cite{agelekOrientingEdgesUnstructured2017,homolyaParallelEdgeOrientation2016}.
It is also possible to number unstructured hexahedral meshes in this manner, but the algorithm cannot be performed in parallel and it will not work for all possible meshes~\cite{agelekOrientingEdgesUnstructured2017}.
In particular, if a subset of the mesh forms a Möbius strip then it is not orientable.

The mesh renumbering approach is also unsuitable for other cell types (e.g. pyramids) or \textit{mixed mesh} methods.

It is clear that a renumbering strategy is not a sufficiently general solution to the orientation problem.
Therefore the stencil library abstraction needs to permit for different orientations.

\subsubsection{The next step: Permuting scalar \glspl{dof}}

As shown in Figures~\ref{fig:orient_basic_ref} and~\ref{fig:orient_basic_flip}, loading scalar data for cells with flipped edges will result in the data being packed in the wrong order.
To avoid this, the \glspl{dof} need to be \textit{permuted} prior to packing.
This permutation can actually be precomputed and form part of the indirection maps used to address the stencils.

\subsubsection{All the way there: Transforming vector \glspl{dof}}

The above approach works for all scalar-valued function spaces, but is insufficient for vector-valued ones due to the need for transformations of the \gls{dof} values themselves.
For example, as described above, the flip shown in Figures~\ref{fig:orient_vector_ref} and~\ref{fig:orient_vector_flip} mean that the \glspl{dof} need to be scaled by -1 prior to packing.
It is not possible to store non-permutation transformations inside an indirection map and so the stencil application needs to implement a separate stage to resolve these orientation transformations.
This is the approach used by Basix~\cite{scroggsConstructionArbitraryOrder2021,scroggsBasixRuntimeFinite2022}, part of the FEniCSx finite element software suite~\cite{loggAutomatedSolutionDifferential2012,AlnaesBlechta2015a}.
However, to our knowledge, this is not performed by any existing general purpose stencil library.

\subsection{Methods for performance optimisation}

In this section, we review some of the common bottlenecks in massively parallel simulation codes and describe some general ways for quantifying and improving performance.
In particular, we focus on challenges for maximising parallel efficiency and the importance of the roofline model for choosing appropriate optimisations.

\subsubsection{Achieving parallel efficiency}
\label{sec:background_perf_efficiency}

In order to run massive simulations, codes must be able to exploit the vast amounts of parallelism afforded to them by modern supercomputers.
With the building of ever larger and more parallel machines (especially since we are at the ``dawn of exascale''), this is becoming both more important to get right and more challenging to do so.

To quantify a code's effectiveness in parallel, one typically measures its \textit{parallel efficiency} under either \textit{strong-} or \textit{weak-scaling}:

\begin{paragraph}{Strong-scaling}
  Strong-scaling describes the behaviour of a code as the number of processes increases for a problem of \textit{fixed size}.
  In a strong-scaling investigation, perfect efficiency (unity) would be achieved if the time-to-solution on $p$ processors was $p$ times smaller than the time-to-solution for a single process.
  A code would be considered to have `good' strong-scaling if it retained high efficiency (e.g. 80\%) at small problem sizes (e.g. 5000 \glspl{dof} per process for \gls{fem}).

  If the efficiency is low, this suggests that there are sizeable portions of the code that are getting run in serial on each process, rather than being shared across all processes.
  To improve efficiency, therefore, one should focus on either reducing this overhead or distributing the work more effectively between processes.
\end{paragraph}

\begin{paragraph}{Weak-scaling}
  Weak-scaling differs from strong-scaling in that, rather than describing the decrease in time-to-solution for a problem of fixed size, it describes the behaviour of the code over a \textit{range of problem sizes}, where the size of the problem scales linearly with the number of processors.
  In this case, perfect efficiency is achieved if the time-to-solution remains fixed with increasing parallelism.
  For a code to have `good' weak-scaling, it would need to have a high efficiency (e.g. 80\%) even when run on very large clusters.

  If a code has poor weak-scaling, this suggests that there are algorithmic problems regarding how the problem is distributed among processors.
  For example, a parallel algorithm that required frequent all-to-all broadcast messages would have poor weak-scaling because this would increase in cost with the number of processors.
\end{paragraph}

\vspace{1em}

Taken together, these two metrics provide a relatively good indicator as to the suitability of a code for running on massively parallel computers.
More informative measures of performance that take into account things such as convergence rates also exist~\cite{changComparativeStudyFinite2018}, but we eschew such approaches here because they fall under the remit of the design of the stencil itself, which is not the focus for this work.

\subsubsection{Maximising floating-point throughput}
\label{sec:background_perf_flops}

\begin{figure}
  \input{figures/roofline.pgf}
  \caption{
    A simplified roofline model.
    The points A, B and C represent computational kernels with different performance characteristics.
  }
  \label{fig:roofline}
\end{figure}

% flame graphs? - need to find hotspots

Once we have a code that scales well, the next step is to optimise performance for a single process.
In order to do this, one should first profile the code and compare its performance to the theoretical limits of the hardware using a \textit{roofline plot}~\cite{williamsRooflineInsightfulVisual2009}.
A roofline plot provides both a good termination criterion for the optimisation process - you can't go faster than the hardware! - and also guides the developer as to what the performance bottlenecks might be and thus which optimisations might be usefully applied.

To begin with, one needs to measure the performance-critical kernels in the application to determine their floating-point throughput (the higher the better) and \textit{arithmetic intensity}.
Arithmetic intensity is a ratio of the number of \glspl{flop} performed per byte of memory accessed and it indicates whether or not a kernel is likely to be \textit{memory-bound}, where maximum throughput is limited by memory access speeds, or \textit{compute-bound}, where it is limited by the speed of the chip itself.
In general, operations such as square roots and divisions typically require a large number of \glspl{flop}, yielding a high arithmetic intensity, while simple streaming access to an array (e.g. \clang{z[i] = x[i] + y[i]}) read a large amount of data compared to the number of \glspl{flop} performed and have a correspondingly low arithmetic intensity.

Having recorded the floating-point throughput and arithmetic intensity, the kernels can now be added to a roofline plot.
An example plot is shown in Figure~\ref{fig:roofline}.
To the left of the plot, where arithmetic intensity is low, throughput is dependent upon the rate at which data can be supplied to the chip and so the ceilings are given by the bandwidths of the various levels in the memory hierarchy.
By contrast, to the right the arithmetic intensity is high and so the performance limits are prescribed by the peak floating-point throughput of the chip.
Two lines are shown to demonstrate the fact that vectorisation can increase performance over standard scalar operations.

Looking at the example kernels shown in red in the figure, we can demonstrate the utility of roofline plots in the optimisation effort:

\begin{itemize}
  \item
    Kernel \textit{A} has low arithmetic intensity and so the maximum achievable throughput is well below theoretical peak.
    It lies fairly close to the main memory bandwidth ceiling and so fruitful optimisation efforts could include: modifying the data access patterns to better utilise cache memory, or making algorithmic modifications to increase the arithmetic intensity.

  \item
    Kernel \textit{B} has high arithmetic and so should be able to achieve close to peak performance.
    It currently sits well below the theoretical limits and is therefore an excellent candidate for further optimisation.
    Since the code cannot be memory-bound, optimisation efforts should focus on compute-type optimisations such as loop unrolling, rather than data ones.

  \item
    Lastly, kernel \textit{C} already achieves very close to peak throughput.
    It is not a good candidate for optimisation since the potential for performance improvements is low.
\end{itemize}

Within the context of stencil computations, the key observation that can be made regarding the classification of kernels into being either compute- or memory-bound is that, in the main, only memory-bound optimisations are worthwhile pursuing.
This is because, if compute-bound, the vast majority of the code runtime will be spent inside the stencil computation, and hence all of the `hot loops' that impact performance will be found inside it.
On the other hand, if we are memory-bound, then most of the time will be spent inside the packing and unpacking code of the stencil library and there are genuine opportunities for optimisation.
Therefore, in general, optimisations for stencil languages should focus on improving memory accesses.

We remark briefly that notable exceptions to this rule are inter-element vectorisation and GPU offloading.
In both cases this would be implemented in a stencil library by, instead of looping over the iteration set one at a time, looping over multiple entities simulatenously, one per vector lane/GPU thread.
This can improve the performance of compute-bound codes by allowing the chip to execute multiple operations per clock cycle.

In the case of inter-element vectorisation, in Figure~\ref{fig:roofline} this would correspond to shifting the kernel from under the ``Peak scalar throughput" ceiling to the ``Peak vector throughput" one.
Such an approach has been implemented in (a branch of) \pyop2, and demonstrated to be performant~\cite{sunStudyVectorizationMatrixfree2020}.
Similarly, some preliminary work on adding GPU offloading support to \pyop2 is ongoing~\cite{fenics2021-kulkarni}.

\subsection{Optimisations for stencil computations}
\label{sec:background_opt}

\subsubsection{Locality optimisations}
\label{sec:background_opt_locality}

From the roofline in Figure~\ref{fig:roofline} one can see that, for a memory-bound code (low arithmetic intensity), dramatic speedups may be achieved by changing the level in the memory hierarchy at which the kernel operates.
In this simplified figure this corresponds to being limited by ``Peak cache bandwidth'' instead of ``Peak main memory bandwidth'' (in reality chips have multiple layers of cache memory with different capacities and performance characteristics).

Cache levels have a small capacity, and so in order to exploit the faster data accesses the \textit{working-set size} of the problem must be reduced to fit inside a particular cache level.
The working-set size describes the amount of data that must be on hand to perform a computation.
If this data volume exceeds the capacity of a cache level then memory performance will be driven by the cost of retrieving data from the next-fastest, next-smallest level of the memory hierarchy.
Memory access speeds can differ by an order of magnitude between levels and so the working-set size is a primary consideration for memory-bound applications.
One of the key ways to reduce the working-set size is to maximise \textit{data locality}.

% I don't like this bit at all...
To try and minimise the number of accesses made to main memory, caches assume that application data exhibit both \textit{spatial locality} and \textit{temporal locality}.
Spatial locality refers to the fact that if a particular piece of data is used by the application, then it is likely that neighbouring data will also be needed.
Caches therefore load data in contiguous chunks termed \textit{cache lines} so data adjacent to the target also get loaded into the cache.
Temporal locality in caches is the assumption that loaded data may be needed multiple times.
To exploit this, cache lines persist in the cache until eviction by new data.

To optimise performance, one must make \textit{data locality} optimisations, that is, optimisations that ensure greater utilisation of the additional entries loaded per cache line as well as trying to avoid repeated loads of the same data if repeated accesses are required.

Common data locality optimisations for stencil-like applications include \textit{tiling}, where a multi-dimensional iteration domain is subdivided into small \textit{tiles} to exploit data reuse between the stencils~\cite{irigoinSupernodePartitioning1988,ramanujamTilingMultidimensionalIteration1992}, and \textit{kernel fusion}, where separate stencils are executed in a single loop to take advantage of the data shared between them~\cite{darteComplexityLoopFusion2000}.
\textit{Time tiling} is a combination of the two where the iteration domain is tiled and then multiple stencils are applied in turn over each tile~\cite{luporiniAutomatedTilingUnstructured2019}..
In the same way as kernel fusion this aims to exploit temporal locality by reusing data between the stencils.

For unstructured meshes, a common optimisation is to renumber the mesh entities such that all entities in the stencil are `close', hence making use of spatial locality.
It also helps with temporal locality since \glspl{dof} on faces will be reused between iterations.
The \gls{rcm} algorithm is a common `cache-oblivious' way of reordering an unstructured mesh~\cite{cuthillReducingBandwidthSparse1969,langeEfficientMeshManagement2016}.

\subsubsection{Exploiting mesh structure}
\label{sec:background_structure}

\begin{figure}
  \centering
  \begin{tikzpicture}[scale=1.5]
    % start with 2 base triangles (4 vertices)
    \tkzDefPoint(0,.5){v0v0}
    \tkzDefPoint(2,0){v1v0}
    \tkzDefPoint(1.2,1){v2v0} % this one is invisible
    \tkzDefPoint(3.2,.5){v3v0}

    % now figure out the extruded points (3 columns)
    \tkzDefShiftPoint[v0v0](90:1){v0v1}
    \tkzDefShiftPoint[v0v0](90:2){v0v2}
    \tkzDefShiftPoint[v0v0](90:3){v0v3}

    \tkzDefShiftPoint[v1v0](90:1){v1v1}
    \tkzDefShiftPoint[v1v0](90:2){v1v2}
    \tkzDefShiftPoint[v1v0](90:3){v1v3}

    \tkzDefShiftPoint[v2v0](90:1){v2v1}
    \tkzDefShiftPoint[v2v0](90:2){v2v2}
    \tkzDefShiftPoint[v2v0](90:3){v2v3}

    \tkzDefShiftPoint[v3v0](90:1){v3v1}
    \tkzDefShiftPoint[v3v0](90:2){v3v2}
    \tkzDefShiftPoint[v3v0](90:3){v3v3}

    % base
    \tkzDrawSegments[red!80,dashed,line width=1](v0v0,v2v0 v1v0,v2v0 v2v0,v3v0)

    % draw the vertical lines (v2 is invisible)
    \tkzDrawSegments[line width=1](v0v0,v0v1 v0v1,v0v2 v0v2,v0v3)
    \tkzDrawSegments[line width=1](v1v0,v1v1 v1v1,v1v2 v1v2,v1v3)
    \tkzDrawSegments[dashed,line width=1](v2v0,v2v1 v2v1,v2v2 v2v2,v2v3)
    \tkzDrawSegments[line width=1](v3v0,v3v1 v3v1,v3v2 v3v2,v3v3)

    % horizontal (bottom is red)
    \tkzDrawSegments[red!80,line width=1](v0v0,v1v0 v1v0,v3v0)
    \tkzDrawSegments[line width=1](v0v1,v1v1 v1v1,v3v1)
    \tkzDrawSegments[line width=1](v0v2,v1v2 v1v2,v3v2)
    \tkzDrawSegments[line width=1](v0v3,v1v3 v1v3,v3v3)

    \tkzDrawSegments[dashed,line width=1](v0v1,v2v1 v1v1,v2v1 v2v1,v3v1)
    \tkzDrawSegments[dashed,line width=1](v0v2,v2v2 v1v2,v2v2 v2v2,v3v2)

    \tkzDrawSegments[line width=1](v0v3,v2v3 v2v3,v3v3)
    \tkzDrawSegment[line width=1](v1v3,v2v3)

    \tkzDrawPoints[red!80,size=4pt](v0v0,v1v0,v2v0,v3v0)
    \tkzDrawPoints[size=4pt](v0v1,v0v2,v0v3,v1v1,v1v2,v1v3,v2v1,v2v2,v2v3,v3v1,v3v2,v3v3)
  \end{tikzpicture}
  \caption{A sample extruded mesh formed by uniformly extruding a base mesh of two triangles (highlighted in red).}
  \label{fig:extruded}
\end{figure}

As well as reordering data to reduce the \textit{effective} working-set size, one can sometimes exploit the structure of the mesh itself to reduce the actual working-set size by reducing the volume of data needed per computation.

Meshes can (usually) be classified into one of two types: \textit{structured} or \textit{unstructured}.
The fundamental difference between the two is that structured grids can determine their neighbouring entities without needing to tabulate everything individually.
This means that data accesses have the form \clang{x[f(i)]} instead of something like \clang{x[map[i]]} that is required for unstructured meshes.

Structured meshes are known to have faster access to data than unstructured meshes for the following reasons:

\begin{itemize}
  \item
    \textit{Beneficial data layout} \\
    Since the data layout of a structured mesh is completely regular, we get a higher rate of cache hits as we iterate over the mesh.
    This is because there will be data reuse on the face where the two cells meet and also the hardware prefetcher will load cache lines for us.

  \item
    \textit{Less memory is required per data access} \\
    Since lookup tables are not required for locating data, the total volume of data required from main memory is reduced.
    This reduces the working-set size of the problem.

  \item
    \textit{Smaller working-set size} \\
    We need less data in order to compute a single stencil.
    If this volume exceeds the size of a particular level in the cache hierarchy then the next one down needs to get used and this will have a bandwidth that is orders-of-magnitude slower.

  \item
    \textit{Aids software prefetching} \\
    If the compiler can see that data is accessed in a particular pattern, it can emit prefetch instructions such that data will already be in the cache.
\end{itemize}

These benefits have motivated the design of \textit{partially-structured} meshes, where only portions of the mesh are structured.
Examples include: block-structured meshes, regularly refined meshes and extruded.

An example extruded mesh is shown in Figure~\ref{fig:extruded}.
Users start with an unstructured `base' mesh - here two triangles - that is \textit{extruded} to produce layers of triangular prisms.
The mesh has `partial structure' in that the data layout is regular up each column, but irregular for the base mesh.
As the number of layers increases, the cost of accessing extruded meshes has been shown to approach that of structured meshes~\cite{berceaStructureexploitingNumberingAlgorithm2016}.

Of the reasons for improved performance described above, we would like to emphasise that the first of these, the ``beneficial data layout", is by far the most important, and in particular that the absence of indirection maps reducing the data volume has only a small effect.

The reason for this is that the difference in data volume between the meshes usually at most 25\%.
To demonstrate, consider a stencil code looping over the cells of a structured mesh where, for each cell, there are $p$ points accessed and $d$ \glspl{dof} per point.
The total (minimum) amount of memory accessed is then given by
\begin{equation*}
  D_S = n_c \cdot p \cdot d \cdot 8 \cdot 2,
\end{equation*}
where $n_c$ is the number of cells, the 8 comes from the fact that each \gls{dof} is 8 bytes, and the 2 is assuming that we read from one array and write to another.
In the unstructured case, the data volume is the same as before plus the size of the indirection map:
\begin{equation*}
  D_U = D_S + n_c \cdot p \cdot 4.
\end{equation*}
A factor of 4 is required instead of 8 because the maps are typically 4 byte integers.
As a fraction of the structured case, the extra data required by the unstructured mesh is therefore given by
\begin{equation*}
  \frac{D_U - D_S}{D_S} = \frac{1}{4d},
\end{equation*}
which, since $d \geq 1$, bounds the extra data volume at 25\% of the structured case.

Taking into consideration that this represents a reasonable worst-case example - stencils frequently admit more than two data structures and often have more than one \gls{dof} per point - the performance benefits of direct addressing are not very dramatic compared with the possible order of magnitude improvements that one can get from a good data ordering.
Though we do address adding support for partially-structured meshes in Section~\ref{sec:future_partialstructure}, initial work on \pyop3 does not focus on it.
